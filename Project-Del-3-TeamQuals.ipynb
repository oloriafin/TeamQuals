{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b528930",
   "metadata": {},
   "source": [
    "Project Deliverable #3: Data Collection and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348898c",
   "metadata": {},
   "source": [
    "This following code loads our dataset and shows the first 5 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51c958-705e-4b02-86b2-5367e7232de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load my dataset\n",
    "df = pd.read_csv(\"Firefox_bugs.csv\")\n",
    "\n",
    "# See the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47b3be",
   "metadata": {},
   "source": [
    "The following code counts the number of duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd6a6f-e3c9-4099-a7d1-b30627939547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total duplicate rows\n",
    "num_dupes = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_dupes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1c96d",
   "metadata": {},
   "source": [
    "Since it showed no duplicates, we specified that the check for duplicated be based on duplicated summary and descriptions. 62 Rows were found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4862add-1ebe-4cd5-83fd-a0b90e5c830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=[\"Summary\", \"Description\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a371e6",
   "metadata": {},
   "source": [
    "This code removes all the duplicates found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573607b-d115-44b0-9abb-8bf66757183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicate rows based on summary and description, if found\n",
    "df = df.drop_duplicates(subset=[\"Summary\", \"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c94c8d-80f6-49e4-9d59-ecfd3136fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=[\"Summary\", \"Description\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8011a0",
   "metadata": {},
   "source": [
    "The next two code excertps installs packages we need to complete our data pre-processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8fa38-94da-41cf-bc40-ed766c526f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c714513-29d2-4b87-882c-b063032ad3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f38e1-501d-4daf-8b21-df1e10af6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9518a8",
   "metadata": {},
   "source": [
    "The code encodes the status and resolution fields and shows results in the first few rows. \n",
    "We use oneHotEncoder for the resolution and status field because order doesnt matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b226ff-26a0-45b3-86ec-ec04d361bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "encodCol = ohe.fit_transform(df[[\"Status\", \"Resolution\"]])\n",
    "\n",
    "df_encoded = pd.concat([\n",
    "    df.reset_index(drop=True),\n",
    "    pd.DataFrame(encodCol.astype(int), columns=ohe.get_feature_names_out([\"Status\", \"Resolution\"]))\n",
    "], axis=1)\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685bc84",
   "metadata": {},
   "source": [
    "This next couple excerpts of code sets the ordinal order for the priority field with P5 being \n",
    "the most critical, encodes it and show results in the first couple rows\n",
    "We dont use a label encoder because it will order alphabetically and in this case P5 is lowest priority \n",
    "so we need to map the order manually to make sure it is right. \n",
    "We use pd.Categorical which is an ordinal encoder that preserves both label and order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675db70a-262f-4da2-afd0-fc66231c352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_order = [\"P5\",\"P4\",\"P3\",\"P2\",\"P1\"]  #lowest â†’ highest\n",
    "priority_map = {p:i for i,p in enumerate(priority_order)}\n",
    "\n",
    "df[\"Priority_encoded\"] = df[\"Priority\"].map(priority_map)\n",
    "\n",
    "unmapped = df.loc[df[\"Priority_encoded\"].isna(), \"Priority\"].unique()\n",
    "print(\"Unmapped values:\", unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec259d-40e7-4b33-9f99-9cbe95fe7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Priority_cat\"] = pd.Categorical(df[\"Priority\"], categories=priority_order, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9dac6-4065-4aab-97f8-986dc649c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Priority\",\"Priority_encoded\"]].head(10)\n",
    "\n",
    "# check to make sure it worked\n",
    "print(df[\"Priority_cat\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f7090-9eef-47d7-be68-243338a5ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Priority\", \"Priority_cat\", \"Priority_encoded\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c54473-e257-45c1-aff6-972e57ad82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e1ec4",
   "metadata": {},
   "source": [
    "This code creates a function to process raw text by removing special characters, transforming to lowercase, tokenizing, stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4aac26-a4bf-4d80-945a-17d59af2dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EN_STOP = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if pd.isna(text) else str(text)\n",
    "\n",
    "    #remove special characters \n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "\n",
    "    #Collapse multiple spaces if exists\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #remove stop words\n",
    "    tokens = [t for t in tokens if t not in EN_STOP]\n",
    "\n",
    "    #stemming\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48bebf",
   "metadata": {},
   "source": [
    "This code concatentes the text and description field into one column and displays the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb2e4e-df9f-4d6d-adad-221214ed329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create the combined Text column\n",
    "df[\"Summary\"] = df[\"Summary\"].fillna(\"\").astype(str)\n",
    "df[\"Description\"] = df[\"Description\"].fillna(\"\").astype(str)\n",
    "df[\"Text\"] = (df[\"Summary\"] + \" \" + df[\"Description\"]).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# 4) Check to see if it worked\n",
    "print(\"New column created:\", \"Text\" in df.columns)\n",
    "print(df[[\"Summary\", \"Description\", \"Text\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4658c5",
   "metadata": {},
   "source": [
    "This code applies our preprocessing function to the newly created column and returns a column with processed tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db595271-169d-4fec-b9ad-db8afdeee0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply tokenization column\n",
    "df[\"processed_tokens\"] = df[\"Text\"].apply(preprocess_text)\n",
    "df[\"processed_text\"]  = df[\"processed_tokens\"].apply(lambda toks: \" \".join(toks))\n",
    "\n",
    "#Make sure it worked\n",
    "df[[\"Text\", \"processed_tokens\"]].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
